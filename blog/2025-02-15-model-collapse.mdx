---
slug: model-collapse
title: Model Collapse
authors: [bilelsgh]
tags: [ia, actualitÃ©]
---

## Lâ€™invasion silencieuse du contenu IA

Une grande partie du contenu sur Internet est en train de devenir du contenu gÃ©nÃ©rÃ© par IA. Lâ€™intÃ©gration progressive des LLMs (Large Language Models) dans notre quotidien dÃ©mocratise et facilite leur utilisation. Les rÃ©seaux sociaux sont devenus de vÃ©ritables mines dâ€™or pour ce type de contenu : de plus en plus dâ€™utilisateurs confient la rÃ©daction ou la reformulation de leurs posts Ã  ChatGPT ou LeChat de Mistral avant de les publier (peut-Ãªtre que ce post y est passÃ© aussi ? ğŸ‘€). Un exemple frappant est celui de Quora dont le contenu gÃ©nÃ©rÃ© par IA est passÃ© de 2% en 2022 Ã  ~38% en 2024 !
{/* truncate */}

## Un risque systÃ©mique : le **Model Collapse** ğŸ’¥

Le problÃ¨me, câ€™est quâ€™Internet constitue aussi, indirectement, la base des datasets sur lesquels ces modÃ¨les sont entraÃ®nÃ©s. Or, un modÃ¨le qui s'entraÃ®ne uniquement sur des donnÃ©es dites "synthÃ©tiques" perd progressivement des informations sur les Ã©lÃ©ments les moins frÃ©quents, ses rÃ©ponses deviennent plus gÃ©nÃ©riques, et les biais existants s'accentuent. Le modÃ¨le sâ€™effondre, on parle de Model CollapseğŸ’¥.

## Et demain ?

Aujourdâ€™hui, aucun modÃ¨le nâ€™est entraÃ®nÃ© exclusivement sur des donnÃ©es synthÃ©tiques. Mais si la tendance actuelle se poursuit, les datasets d'entraÃ®nement continueront de croÃ®tre et intÃ©greront une part toujours plus importante de contenu gÃ©nÃ©rÃ© par IA. Il sera alors crucial de porter une attention particuliÃ¨re Ã  leur Ã©laboration et Ã  lâ€™identification des contenus synthÃ©tiques, sous peine dâ€™empoisonner nos futurs modÃ¨les.

![alt text](/img/blog/mcollapse.jpg 'model collapse')

## ğŸ“š Sources

1. Forbes - Is AI quietly killing itself â€“ and the Internet? (2024)
2. Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media â€” Zhen Sun
3. AI World Today â€” AI-Generated Content Surges on Social Media: New Study Reveals Startling Trends (2024)
4. AI models collapse when trained on recursively generated data â€” Ilia Shumailov (2024)
5. Position: Will we run out of data? Limits of LLM scaling based on human-generated data â€” P Villalobos (2024)
