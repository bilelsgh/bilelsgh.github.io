---
slug: rl-for-anomalies-detection
title: Un agent de s√©curit√© nos r√©seaux ? L‚Äôapprentissage par renforcement pour d√©tecter les anomalies dans le trafic r√©seau
description: Comment le deep reinforcement learning peut-il aider √† d√©tecter des anomalies dans le trafic r√©seau
authors: [bilelsgh]
tags: [ia, recherche, cyber-securite]
---

[^global]

Notre quotidien se num√©rise rapidement, et les r√©seaux informatiques ne cessent de cro√Ætre en taille et en complexit√©. Le trafic qu‚Äôils transportent est de plus en plus dense et vari√© : transactions bancaires, donn√©es de sant√©, communications priv√©es, etc.
Avec l‚Äôessor des technologies dites intelligentes ‚Äî smart homes, smart cities, m√©decine personnalis√©e ‚Äî le nombre d‚Äôobjets connect√©s devrait d√©passer les 50 milliards d‚Äôici fin 2025. Ces dispositifs, bien qu‚Äôomnipr√©sents, sont souvent peu s√©curis√©s : mises √† jour n√©glig√©es, mots de passe faibles, protocoles obsol√®tes.
Dans ce contexte, les r√©seaux informatiques repr√©sentent une cible de choix pour les cybercriminels, qui profitent d‚Äôune surface d‚Äôattaque en constante expansion. Concevoir des solutions de cybers√©curit√© efficaces et robustes devient ainsi un enjeu crucial, un champ de recherche particuli√®rement dynamique, mais aussi un d√©fi de plus en plus complexe.
{/* truncate */}

## D√©tecter une anomalie
En analysant le trafic, il est possible de d√©tecter des d√©viations de comportement pouvant cacher des cyber attaques. Lorsqu‚Äôon parle de trafic r√©seau, on fait g√©n√©ralement r√©f√©rence √† des flux (ou flows), qui regroupent des paquets partageant des caract√©ristiques communes.
Un flux peut √™tre d√©fini par un ensemble de propri√©t√©s comme les adresses IP, les ports, le protocole de transport, la dur√©e, la taille, etc. mais aussi par des propri√©t√©s plus abstraites [^abs] comme le nombre de paquets, la taille moyenne des paquets, etc.
La d√©tection d‚Äôanomalies est un domaine bien √©tabli, dont les premi√®res approches remontent √† plusieurs d√©cennies. Les m√©thodes par signature ont √©t√© les premiers syst√®mes de d√©tection d'anomalies (IDS). Elles consistent √† comparer le trafic r√©seau √† une base de donn√©es de signatures connues d'attaques et √† lever une alerte s'il y a un match. Cependant, ces m√©thodes sont limit√©es par leur capacit√© √† d√©tecter uniquement les attaques connues, ne peuvent pas s'adapter aux nouvelles menaces et ne sont pas robustes aux l√©g√®res variations.
Rapidement, les m√©thodes bas√©es sur le machine et le deep learning ont √©t√© introduites : en apprenant sur des donn√©es historiques elles surmontent ces limitations et d√©tectent des anomalies plus complexes [^nn].

## Le comportement sur le r√©seau change
Comme pour tout syst√®me d‚Äôapprentissage, la qualit√© et la pertinence des donn√©es utilis√©es jouent un r√¥le fondamental et cela vaut √©galement pour le trafic r√©seau. En plus de la pipeline de pr√©paration classique, il faut s'assurer que les flows soient repr√©sentatifs du comportement actuel du r√©seau pour que les inf√©rences du mod√®le restent pertinentes sur le long terme.
Dragoi, Marius, et al. ont montr√© que plus les ann√©es passaient (Figure 1), plus le comportement du trafic r√©seau √©voluait. Santos Roger quant √† eux mettent en avant l'augmentation des erreurs de diff√©rents mod√®les de d√©tection d'anomalies au fur et √† mesure du temps s'ils ne sont pas r√©-entra√Æn√©s (Figure 2).
Tout cela met en avant le ph√©nom√®ne de concept drift et la n√©cessit√© de r√©-entra√Æner les mod√®les de d√©tection d'anomalies r√©guli√®rement pour que la mod√©lisation du trafic soit correcte, actuelle et que la d√©tection reste pertinente.

<img src="/img/blog/conceptdrift.png" width={"100%"}/>


## L'apprentissage par renforcement
Mais entra√Æner r√©guli√®rement un mod√®le demande du temps, de la puissance de calcul et une implication humaine r√©guli√®re dans le choix et l'√©tiquetage des donn√©es et le choix des moments opportuns auxquels il faut s'entra√Æner.
L'apprentissage par renforcement, une branche du machine learning, propose une approche diff√©rente. Il apprend √† partir d'interactions avec l'environnement et re√ßoit des r√©compenses ou des p√©nalit√©s en fonction de ses actions. En ayant comme objectif de maximiser les r√©compenses futures, il va apprendre √† optimiser ses d√©cisions au fil du temps. Lorsqu'il est appliqu√© √† la d√©tection d'anomalies, ou d'autres domaines dans lesquels l'espace des √©tats est tr√®s grand, un r√©seau de neurones est utilis√© pour approximer la fonction de valeur, qui √©value la qualit√© des actions prises par l'agent dans un √©tat donn√©.
L'environement est d√©fini par le trafic r√©seau que l'agent observe. Il prend des actions (comme lever une alerte ou non) et re√ßoit des r√©compenses en fonction de la pertinence de ses actions.

<img src="/img/blog/rl_ad.png" width={"50%"}/>

## Une fonction de r√©compense pour des cas d'usage r√©alistes
La fonction de r√©compense est alors au coeur de l'apprentissage par renforcement. Elle guide l'agent vers des comportements souhait√©s sans les formuler math√©matiquement [^maths]. Dans le contexte de la d√©tection d'anomalies, la fonction de r√©compense doit √™tre soigneusement con√ßue pour refl√©ter les objectifs de s√©curit√© et les priorit√©s op√©rationnelles.
Si la fonction de r√©compense est trop simple, l'agent peut apprendre des comportements ind√©sirables et l'objectif de maximisation de la r√©compense peut jouer en notre d√©faveur, c'est ce qu'on appelle l'effet Cobra.

:::info
Dans les anciennes colonies britanniques en Inde, le gouvernement a offert une r√©compense pour chaque cobra mort afin d‚Äô√©radiquer leur infestation. Au lieu de diminuer, la population a commenc√© √† √©lever des cobras pour les tuer ensuite et toucher l‚Äôargent.
:::

Dans notre cas, si l'agent re√ßoit une r√©compense trop grande pour chaque alerte lev√©e, il peut devenir trop sensible et constamment lever des alertes pour obtenir un maximum de r√©compense, ce qui entra√Ænerait de nombreux faux positifs. La fonction de r√©compense ne fait pas partie int√©grante de l‚Äôenvironnement : c‚Äôest au concepteur de l‚Äôagent de la d√©finir avec soin, en choisissant pr√©cis√©ment les situations dans lesquelles une r√©compense doit √™tre attribu√©e, et ce qu‚Äôelle repr√©sente r√©ellement.

:::info
Introduire des connaissances suppl√©mentaires dans la fonction de r√©compense peut aider l'agent √† converger plus rapidement, on parle de Reward Shaping.
:::

La plupart du temps dans l'√©tat de l'art, les fonctions de r√©compenses sont bas√©es sur des √©tiquettes emp√™chant l'agent d'apprendre en continue et de mani√®re autonome dans un environnement r√©el.
Dans mes travaux, je me penche sur l'√©laboration d'une fonction de r√©compense qui permettrait √† l'agent d'apprendre sur du trafic nouveau et sans √©tiquettes, notamment en s'appuyant sur des techniques de clustering qui permettraient √† des agents  de s‚Äôadapter aux nouvelles formes de trafic sans supervision humaine, mais j‚Äôaurai s√ªrement l‚Äôoccasion d‚Äôy revenir plus en d√©tail plus tard üòâ 

## üìö Sources

1. Dragoi, Marius, et al. "Anoshift: A distribution shift benchmark for unsupervised anomaly detection." Advances in Neural Information Processing Systems 35 (2022): 32854-32867.
2. Reinforcement Learning for Intrusion Detection: More Model Longness and Fewer Updates - Santos Roger 2022
3. Huang and al. ‚ÄòDeep Learning Advancements in Anomaly Detection: A Comprehensive Survey‚Äù (2025)
4. Tawalbeh, Lo‚Äôai, et al. "IoT Privacy and security: Challenges and solutions." Applied Sciences 10.12 (2020): 4102.
5. Khraisat, Ansam, et al. "Survey of intrusion detection systems: techniques, datasets and challenges." Cybersecurity 2.1 (2019): 1-22.

[^global]: grave bien, bon niveau de vulgarisation je trouve, tu peux peut-etre ajouter des liens externes e.g. cobra, reward shaping
[^abs]: je trouve que nb paquets / taille ca fait pas tres abstrait
[^nn]: j'aurais ajoute qu'ils permettent de generaliser et d'interpoler
[^maths]: je vois l'idee mais, sauf si je dis n'imp, c'est un peu maladroit pcq tu design qd meme le type de fonction que c'est non ? i.e. polynomiale, avec un decay ou non, etc
